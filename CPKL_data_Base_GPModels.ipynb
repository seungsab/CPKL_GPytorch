{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# plt.style.use(['dark_background', 'bmh'])\n",
    "plt.style.use(['seaborn-v0_8-whitegrid', 'bmh'])\n",
    "font = {'family' : 'monospace',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 12}\n",
    "plt.rc('font', **font)\n",
    "plt.rc('axes', facecolor='w')\n",
    "plt.rc('figure', facecolor='w')\n",
    "plt.rc('figure', figsize= (6, 3), dpi=150)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "dir_fitted_result = './fitted_GPModels'\n",
    "os.makedirs(dir_fitted_result, exist_ok = True)\n",
    "\n",
    "dir_trainset = './trainset'\n",
    "os.makedirs(dir_trainset, exist_ok = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Examples for sample size**\n",
    "- For 1d-Dataset: `torch.Size([20]) torch.Size([20]) torch.Size([100]) torch.Size([100])`  \n",
    "- For 2d-dataset: `torch.Size([20, 2]) torch.Size([20]) torch.Size([100, 2]) torch.Size([100])`  \n",
    "- For nd-dataset: `torch.Size([20, n]) torch.Size([20]) torch.Size([100, n]) torch.Size([100])`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Define functions for case studies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.kernels import RBFKernel, LinearKernel, RQKernel, MaternKernel, CosineKernel, PeriodicKernel\n",
    "\n",
    "def define_base_kernels(train_x, traindata_info):\n",
    "\n",
    "    # Set active dimensions\n",
    "    if train_x.dim() == 1:\n",
    "        n_param_hyp = 1\n",
    "    else:\n",
    "        n_param_hyp = train_x.shape[1]\n",
    "        \n",
    "    action_dims = torch.tensor([i for i in range(0, n_param_hyp)])\n",
    "\n",
    "    lengthscale_prior = gpytorch.priors.NormalPrior(\n",
    "        traindata_info['med_dist_x'], traindata_info['max_dist_x'] - traindata_info['min_dist_x'])\n",
    "    variance_prior = gpytorch.priors.NormalPrior(\n",
    "        traindata_info['med_dist_y'], traindata_info['max_dist_y'] - traindata_info['min_dist_y'])\n",
    "    periodlength_prior = gpytorch.priors.NormalPrior(\n",
    "        traindata_info['med_dist_x'], traindata_info['max_dist_x'] - traindata_info['min_dist_x'])\n",
    "\n",
    "    period_length_constraint = gpytorch.constraints.Interval(\n",
    "        0.5*traindata_info['min_dist_x'], traindata_info['max_dist_x']*1.5)\n",
    "\n",
    "    for i in range(n_param_hyp):\n",
    "        if i == 0:\n",
    "            lin_ken = LinearKernel(active_dims=i, variance_prior=variance_prior, variance_constraint=gpytorch.constraints.Interval(\n",
    "                0.5*traindata_info['min_dist_y'], traindata_info['max_dist_y']*1.5))\n",
    "\n",
    "            # Samller values of period_length means higher frequency in signals\n",
    "            cos_ken = CosineKernel(active_dims=i, period_length_prior=periodlength_prior, period_length_constraint=gpytorch.constraints.Interval(\n",
    "                0.5 * traindata_info['min_dist_x'], traindata_info['max_dist_x'] * 1.5))\n",
    "            cos_ken1 = CosineKernel(active_dims=i, period_length_prior=periodlength_prior,\n",
    "                                    period_length_constraint=gpytorch.constraints.LessThan(0.5*traindata_info['min_dist_x']))\n",
    "\n",
    "        else:\n",
    "            lin_ken *= LinearKernel(active_dims=i, variance_prior=variance_prior, variance_constraint=gpytorch.constraints.Interval(\n",
    "                0.5*traindata_info['min_dist_y'], traindata_info['max_dist_y']*1.5))\n",
    "            cos_ken *= CosineKernel(active_dims=i, period_length_prior=periodlength_prior, period_length_constraint=gpytorch.constraints.Interval(\n",
    "                0.5 * traindata_info['min_dist_x'], traindata_info['max_dist_x'] * 1.5))\n",
    "            cos_ken1 *= CosineKernel(active_dims=i, period_length_prior=periodlength_prior,\n",
    "                                    period_length_constraint=gpytorch.constraints.LessThan(0.5*traindata_info['min_dist_x']))\n",
    "\n",
    "    # define the kernels\n",
    "    base_kernels = {\n",
    "        'rbf': lambda: RBFKernel(active_dims=action_dims, ard_num_dims=len(action_dims), lengthscale_prior=lengthscale_prior),\n",
    "        'lin': lambda: lin_ken,\n",
    "        # 'cos': lambda: cos_ken,\n",
    "        # 'cos1': lambda: cos_ken1,\n",
    "        # 'per': lambda: PeriodicKernel(active_dims=action_dims, ard_num_dims=len(action_dims),\n",
    "        #                             period_length_prior=periodlength_prior, lengthscale_prior=lengthscale_prior,\n",
    "        #                             period_length_constraint = period_length_constraint),\n",
    "        'rq': lambda: RQKernel(active_dims=action_dims, ard_num_dims=len(action_dims), lengthscale_prior=lengthscale_prior,),\n",
    "        'mat12': lambda: MaternKernel(nu=0.5, active_dims=action_dims, ard_num_dims=len(action_dims), lengthscale_prior=lengthscale_prior),\n",
    "        'mat32': lambda: MaternKernel(nu=1.5, active_dims=action_dims, ard_num_dims=len(action_dims), lengthscale_prior=lengthscale_prior),\n",
    "        'mat52': lambda: MaternKernel(nu=2.5, active_dims=action_dims, ard_num_dims=len(action_dims), lengthscale_prior=lengthscale_prior),\n",
    "    }\n",
    "\n",
    "    # base_likelihood = lambda: gpytorch.likelihoods.GaussianLikelihood(noise_constraint=gpytorch.constraints.Positive())\n",
    "\n",
    "    def base_likelihood(): return gpytorch.likelihoods.GaussianLikelihood(\n",
    "        noise_constraint=gpytorch.constraints.GreaterThan(1e-3))\n",
    "\n",
    "    # Assume: likelihood noise constraint was defined so that 1e-4 is within range.\n",
    "    likelihood0 = base_likelihood()\n",
    "    if 0:\n",
    "        # Some small value, but don't make it too small or numerical performance will suffer. I recommend 1e-4.\n",
    "        likelihood0.noise = 1e-4\n",
    "        # Mark that we don't want to train the noise.\n",
    "        likelihood0.noise_covar.raw_noise.requires_grad_(False)\n",
    "\n",
    "    return base_kernels, likelihood0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Util_CPKL import *\n",
    "\n",
    "def run_experimental_data_for_CPKL(trainset, testset, rand_seed0, options, plot_result_on = False):\n",
    "    # Run CPKL\n",
    "    multi_restart_option = {}\n",
    "    multi_restart_option['sampling'] = 'random'\n",
    "    multi_restart_option['n_eval'] = 20\n",
    "    \n",
    "    # Assign input and output from tuples\n",
    "    train_y_all, train_x_all = trainset\n",
    "    test_y_all, test_x_all  = testset\n",
    "\n",
    "    train_x, test_x = train_x_all.float(), test_x_all.float()\n",
    "    train_y, test_y =  train_y_all.float(), test_y_all.float()\n",
    "    \n",
    "    # get information trainint set for initializing hyper-parameters\n",
    "    traindata_info = initialize_hyperparameters(train_x, train_y)\n",
    "    multi_restart_option['traindata_info'] = traindata_info\n",
    "    \n",
    "    # Define base_kernels\n",
    "    base_kernels, likelihood0 = define_base_kernels(train_x, traindata_info)\n",
    "    best_model, best_models, best_metrics, best_index = RUN_CPKL(train_x, train_y, likelihood0, base_kernels,\n",
    "                                                    test_x=test_x, test_y=test_y, multi_restart_option=multi_restart_option,\n",
    "                                                    fix_value_learned=True, max_depth=options['max_depth'], rand_seed0=rand_seed0,\n",
    "                                                    training_iter=options['training_iter'], standardize=options['standardize'], \n",
    "                                                    tolerance=options['tolerance'], plot_intermediate_on=False)\n",
    "    \n",
    "    best_metrics = torch.tensor(best_metrics)\n",
    "    \n",
    "    mae, mse = plot_result(best_model, test_x, test_y, plot_on = plot_result_on)\n",
    "\n",
    "    return best_model, best_models, best_metrics, best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_backbone_model(train_x, train_y, kernel_str, options):\n",
    "    # get information trainint set for initializing hyper-parameters\n",
    "    traindata_info = initialize_hyperparameters(train_x, train_y)\n",
    "\n",
    "    # Define base_kernels\n",
    "    base_kernels, likelihood0 = define_base_kernels(train_x, traindata_info)\n",
    "    kernel_type_splitted = kernel_str.replace('(', '')\n",
    "    kernel_type_splitted = kernel_type_splitted.replace(')', '')\n",
    "    kernel_type_splitted = kernel_type_splitted.replace('+', ' + ')\n",
    "    kernel_type_splitted = kernel_type_splitted.replace('*', ' * ')\n",
    "    kernel_type_splitted = kernel_type_splitted.split(' ')\n",
    "\n",
    "    # Define Learned kernels => it can be single or compositional kernel\n",
    "    if len(kernel_type_splitted) == 1: # Single kernel\n",
    "        kernel_ = base_kernels[kernel_type_splitted[0]]()\n",
    "        \n",
    "    else: # Composition using two more kernels\n",
    "        kernel_ = base_kernels[kernel_type_splitted[0]]()\n",
    "        for key in kernel_type_splitted[1:]:\n",
    "            if key in ['*', '+']:\n",
    "                compotional_rule = key\n",
    "            else:\n",
    "                if compotional_rule == '+':\n",
    "                    kernel_ = kernel_ + base_kernels[key]()\n",
    "                else:\n",
    "                    kernel_ = kernel_ * base_kernels[key]()\n",
    "\n",
    "    # Construct GPModel based on the kernel learned before\n",
    "    model_ = ExactGPModel(deepcopy(train_x), deepcopy(train_y), kernel_, kernel_str, deepcopy(likelihood0), options).float()\n",
    "\n",
    "    return model_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training options**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {'training_iter': 100, 'max_depth': 15,\n",
    "            'standardize': True, 'tolerance': 1e-3}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Construct GPModels using whole LF-data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = 'LF'\n",
    "fn_lists_LF = [\n",
    "    f'simplebeam_strain_{data_type}0.csv',\n",
    "    f'simplebeam_strain_{data_type}1.csv',\n",
    "    f'simplebeam_strain_{data_type}2.csv',\n",
    "    f'slab1_strain_{data_type}.csv',\n",
    "    f'slab2_strain_{data_type}.csv',\n",
    "    f'Yongsu_strain_{data_type}.csv',\n",
    "]\n",
    "\n",
    "if 0:\n",
    "    rand_seed0, plot_on = 1234, False\n",
    "\n",
    "    for fn in fn_lists_LF:\n",
    "        fn_str = fn.split('_')\n",
    "\n",
    "        # Import HF-data\n",
    "        df = pd.read_csv('./Data/' + fn)\n",
    "        if df.shape[1] == 2: # 1d\n",
    "            i_loads, input_dim = np.arange(df.shape[1] - 1), 1\n",
    "        else:\n",
    "            i_loads, input_dim = np.arange(df.shape[1] - 2), 2\n",
    "\n",
    "        print(f\"List for Loading Case: {i_loads}\")\n",
    "\n",
    "        for i_load in i_loads:\n",
    "            # Set testdata\n",
    "            if input_dim == 1: # 1D\n",
    "                trainset = (torch.tensor(df.iloc[:, i_load].values), torch.tensor(df.iloc[:, -input_dim].values))\n",
    "                testset = (torch.tensor(df.iloc[:, i_load].values), torch.tensor(df.iloc[:, -input_dim].values))\n",
    "            else: # 2D\n",
    "                testset = (torch.tensor(df.iloc[:, i_load].values), torch.tensor(df.iloc[:, -input_dim:].values))\n",
    "                trainset = (torch.tensor(df.iloc[:, i_load].values), torch.tensor(df.iloc[:, -input_dim:].values))\n",
    "\n",
    "            # Fit GPModel using subsets of HF-data\n",
    "            print(f\"# Case : {i_load}\") # df_tmp.shape[0]\n",
    "            \n",
    "            plot_on = True\n",
    "            best_model, best_models, best_metrics, best_index = run_experimental_data_for_CPKL(deepcopy(trainset), deepcopy(testset), rand_seed0, options, plot_on)\n",
    "            mae, mse = best_metrics[best_index, :]\n",
    "            \n",
    "            # Save best and worst GPModels\n",
    "            fn_sv = f\"Best_{fn_str[0]}_{fn_str[-1].split('.')[0]}_Case{i_load}.pth\"\n",
    "            torch.save(best_model.state_dict(), dir_fitted_result + f'/{fn_sv}')\n",
    "            \n",
    "            # Save trainingset\n",
    "            fn_train_sv = f\"{fn_str[0]}_{fn_str[-1].split('.')[0]}_Case{i_load}.pkl\"\n",
    "            values_saved = {\n",
    "                'trainingset': trainset, 'testset': testset, 'CPKL_str': best_model.kernel_type\n",
    "                }\n",
    "            \n",
    "            with open(dir_trainset + '/' + fn_train_sv, 'wb') as file:\n",
    "                pickle.dump(values_saved, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Reconstruct GPModels using LF-data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_load_cases = [1, 1, 1, 2, 1, 4]\n",
    "Best_GPModel_LF = {}\n",
    "for fn, i_load_case in zip(fn_lists_LF, i_load_cases):\n",
    "    fn_str = fn.split('_')\n",
    "\n",
    "    for i_load in range(i_load_case):\n",
    "        fn_sv = dir_fitted_result + f\"/Best_{fn_str[0]}_{fn_str[-1].split('.')[0]}_Case{i_load}.pth\" # for learned parameters\n",
    "        fn_train_sv = dir_trainset + f\"/{fn_str[0]}_{fn_str[-1].split('.')[0]}_Case{i_load}.pkl\" # for dataset    \n",
    "\n",
    "        # 1. Construct backbone model for initialization\n",
    "        # Load datasets\n",
    "        with open(fn_train_sv, 'rb') as file:\n",
    "            dataset = pickle.load(file)\n",
    "\n",
    "        trainset, testset = dataset['trainingset'], dataset['testset']\n",
    "        kernel_str = dataset['CPKL_str']\n",
    "\n",
    "        # Construct a backbone GPModel\n",
    "        train_x, train_y = trainset[1].float(), trainset[0].float()\n",
    "        best_model_ = reconstruct_backbone_model(train_x, train_y, kernel_str, options)\n",
    "        \n",
    "        # 2. Update initial model using parameter values learned    \n",
    "        state_dict = torch.load(fn_sv) # Load learned values\n",
    "        best_model_.load_state_dict(state_dict)\n",
    "        \n",
    "        Best_GPModel_LF[f\"{fn_str[0]}_{fn_str[-1].split('.')[0]}_Case{i_load}\"] = best_model_\n",
    "\n",
    "        # Make prediction\n",
    "        test_x, test_y = trainset[1].float(), trainset[0].float()\n",
    "        if 0:\n",
    "            print(fn)\n",
    "            mae, mse = plot_result(best_model_, test_x, test_y, plot_on = True)\n",
    "            print(f\"MAE : {mae:.3f} // MSE : {mse:.3f}\")\n",
    "            print(\"%%%%%%%\" * 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Construct GPModels using only HF-data (i.e. subsets)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data_type = 'HF'\n",
    "fn_lists_HF = [\n",
    "    f'simplebeam_strain_{data_type}0.csv',\n",
    "    f'simplebeam_strain_{data_type}1.csv',\n",
    "    f'simplebeam_strain_{data_type}2.csv',\n",
    "    f'slab1_strain_{data_type}.csv',\n",
    "    f'slab2_strain_{data_type}.csv',\n",
    "    f'Yongsu_strain_{data_type}.csv',\n",
    "]\n",
    "\n",
    "ratio_min, ratio_max = 0.3, 0.7\n",
    "i_sample_min, i_sample_max = 6, 10\n",
    "rand_seed0, plot_on = 1234, False\n",
    "n_rep = 10\n",
    "\n",
    "if 0:\n",
    "    for fn in fn_lists_HF:\n",
    "        fn_str = fn.split('_')\n",
    "\n",
    "        # Import HF-data\n",
    "        df = pd.read_csv('./Data/' + fn)\n",
    "        if df.shape[1] == 2: # 1d\n",
    "            i_loads, input_dim = np.arange(df.shape[1] - 1), 1\n",
    "        else:\n",
    "            i_loads, input_dim = np.arange(df.shape[1] - 2), 2\n",
    "\n",
    "        print(f\"List for Loading Case: {i_loads}\")\n",
    "\n",
    "        for i_load in i_loads:\n",
    "            # Set testdata\n",
    "            if input_dim == 1:\n",
    "                testset = (torch.tensor(df.iloc[:, i_load].values), torch.tensor(df.iloc[:, -input_dim].values)) # 1D\n",
    "            else:\n",
    "                testset = (torch.tensor(df.iloc[:, i_load].values), torch.tensor(df.iloc[:, -input_dim:].values)) # 1D\n",
    "\n",
    "            # Loop for generating trainingset via stratified sampling\n",
    "            if input_dim == 1:\n",
    "                i_samples = np.unique(np.linspace(df.shape[0] * ratio_min, df.shape[0] * ratio_max, 5, dtype = int))\n",
    "            else:\n",
    "                i_samples = np.unique(np.arange(i_sample_min, i_sample_max + 1, dtype = int))\n",
    "\n",
    "            print(f\"List for # Samples: {i_samples}\")\n",
    "\n",
    "            all_trainingsets, best_trainingsets, worst_trainingsets = [], [], []\n",
    "            best_kernel_types, worst_kernel_types = [], []\n",
    "            train_set_all_mae, train_set_all_mse = np.full((n_rep,1),np.nan), np.full((n_rep,1),np.nan)\n",
    "            for ind_sample in i_samples: # increase # of HF-data\n",
    "                if input_dim == 1:\n",
    "                    # Get minimal and maximal inputs\n",
    "                    xmin_, xmax_ = testset[-1].min(), testset[-1].max()\n",
    "                    \n",
    "                    # Generate stratified intervals (cut_interval)\n",
    "                    cut_interval = np.linspace(xmin_, xmax_, ind_sample + 1)\n",
    "                    cut_interval[0], cut_interval[-1] = - np.inf, np.inf\n",
    "\n",
    "                    df['cut'] = pd.cut(df['x'], bins = cut_interval, labels = [i for i in range(cut_interval.shape[0] - 1)])\n",
    "                    tmp = df.groupby('cut', group_keys=False)\n",
    "                    \n",
    "                    train_set_mae, train_set_mse = [], []\n",
    "                    train_set, model_list = [], []\n",
    "\n",
    "                    for i_rep in range(n_rep): # repeat random sampling based on stratified intervals\n",
    "                        df_tmp = tmp.apply(lambda x: x.sample(1, replace=True, random_state = rand_seed0 * i_rep))\n",
    "                        df_tmp.drop('cut', axis=1, inplace=True)\n",
    "                        \n",
    "                        trainset = (torch.tensor(df_tmp.iloc[:, i_load].values), torch.tensor(df_tmp.iloc[:, -input_dim].values))\n",
    "\n",
    "                        # Fit GPModel using subsets of HF-data\n",
    "                        print(f\"[{i_rep + 1}/{n_rep}] # sample : {ind_sample} // # Case : {i_load}\") # df_tmp.shape[0]\n",
    "                        best_model, best_models, best_metrics, best_index = run_experimental_data_for_CPKL(deepcopy(trainset), deepcopy(testset), rand_seed0, options, plot_on)\n",
    "                        mae, mse = best_metrics[best_index, :]\n",
    "\n",
    "                        # Append GPModel\n",
    "                        model_list.append(best_model)\n",
    "                        train_set_mae.append(mae.item())\n",
    "                        train_set_mse.append(mse.item())\n",
    "                        train_set.append(trainset)\n",
    "\n",
    "                else: # above 2D\n",
    "                        np.random.seed(rand_seed0) # for reproducibility\n",
    "                        if fn_str[0] == 'slab1': # LF-data at x = 300 was not measured\n",
    "                            df_tmp = df.iloc[3:, :]\n",
    "                            \n",
    "                        else:\n",
    "                            df_tmp = df.iloc[:, :]\n",
    "\n",
    "                        train_set_mae, train_set_mse = [], []\n",
    "                        train_set, model_list = [], []\n",
    "\n",
    "                        for i_rep in range(n_rep): # repeat random sampling based on stratified intervals\n",
    "                            ind_train = np.random.choice(df_tmp.shape[0], ind_sample, replace=False)\n",
    "                            trainset = (torch.tensor(df_tmp.iloc[ind_train, i_load].values), torch.tensor(df_tmp.iloc[ind_train, -input_dim:].values))\n",
    "                                                        \n",
    "                            # Fit GPModel using subsets of HF-data\n",
    "                            print(f\"[{i_rep + 1}/{n_rep}] # sample : {ind_sample} // # Case : {i_load}\") # df_tmp.shape[0]\n",
    "                            best_model, best_models, best_metrics, best_index = run_experimental_data_for_CPKL(deepcopy(trainset), deepcopy(testset), rand_seed0, options, plot_on)\n",
    "                            mae, mse = best_metrics[best_index, :]\n",
    "\n",
    "                            # Append GPModel\n",
    "                            model_list.append(best_model)\n",
    "                            train_set_mae.append(mae.item())\n",
    "                            train_set_mse.append(mse.item())\n",
    "                            train_set.append(trainset)\n",
    "\n",
    "                # Find indices of best and worst model\n",
    "                train_set_mae = np.array(train_set_mae).reshape(-1, 1)\n",
    "                train_set_mse = np.array(train_set_mse).reshape(-1, 1)\n",
    "\n",
    "                ind_best = np.array(train_set_mae).argmin()\n",
    "                ind_worst = np.array(train_set_mae).argmax()\n",
    "\n",
    "                # Save best and worst GPModels\n",
    "                fn_sv = f\"Best_{fn_str[0]}_{fn_str[-1].split('.')[0]}_N{ind_sample}_Case{i_load}.pth\"\n",
    "                torch.save(model_list[ind_best].state_dict(), f'{dir_fitted_result}/{fn_sv}')\n",
    "\n",
    "                fn_sv = f\"Worst_{fn_str[0]}_{fn_str[-1].split('.')[0]}_N{ind_sample}_Case{i_load}.pth\"\n",
    "                torch.save(model_list[ind_worst].state_dict(), f'{dir_fitted_result}/{fn_sv}')\n",
    "                \n",
    "                best_kernel_types.append(model_list[ind_best].kernel_type)\n",
    "                worst_kernel_types.append(model_list[ind_worst].kernel_type)\n",
    "\n",
    "                # Append training data & metrics\n",
    "                all_trainingsets.append(train_set)\n",
    "                best_trainingsets.append(train_set[ind_best])\n",
    "                worst_trainingsets.append(train_set[ind_worst])\n",
    "                if np.isnan(train_set_all_mae).all():\n",
    "                    train_set_all_mae, train_set_all_mse = train_set_mae, train_set_mse\n",
    "                else:\n",
    "                    train_set_all_mae = np.concatenate([train_set_all_mae, train_set_mae], axis = 1)\n",
    "                    train_set_all_mse = np.concatenate([train_set_all_mse, train_set_mse], axis = 1)\n",
    "\n",
    "            df_mae = pd.DataFrame(train_set_all_mae)\n",
    "            df_mae.columns = i_samples\n",
    "            df_mse = pd.DataFrame(train_set_all_mse)\n",
    "            df_mse.columns = i_samples\n",
    "\n",
    "            df_mae.to_csv(dir_trainset + '/' + f\"MAE_{fn_str[0]}_{fn_str[-1].split('.')[0]}_Case{i_load}.csv\")\n",
    "            df_mse.to_csv(dir_trainset + '/' + f\"MSE_{fn_str[0]}_{fn_str[-1].split('.')[0]}_Case{i_load}.csv\")\n",
    "\n",
    "            # Save trainingset\n",
    "            fn_train_sv = f\"{fn_str[0]}_{fn_str[-1].split('.')[0]}_Case{i_load}.pkl\"\n",
    "            values_saved = {\n",
    "                'best_trainingsets': best_trainingsets, 'worst_trainingsets': worst_trainingsets, \n",
    "                'all_trainingsets': all_trainingsets, 'testset': testset,\n",
    "                'best_CPKL_str': best_kernel_types, 'worst_CPKL_str': worst_kernel_types,\n",
    "                }\n",
    "            \n",
    "            with open(dir_trainset + '/' + fn_train_sv, 'wb') as file:\n",
    "                pickle.dump(values_saved, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Reconstruct GPModels using HF-data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0\n",
    "fn = fn_lists_HF[4]\n",
    "\n",
    "i_load_cases = [1, 1, 1, 2, 1, 4]\n",
    "\n",
    "fn_str = fn.split('_')\n",
    "i_load = 0\n",
    "\n",
    "fn_train_sv = dir_trainset + f\"/{fn_str[0]}_{fn_str[-1].split('.')[0]}_Case{i_load}.pkl\" # for dataset\n",
    "\n",
    "# 1. Construct backbone model for initialization\n",
    "# Load datasets\n",
    "with open(fn_train_sv, 'rb') as file:\n",
    "    dataset = pickle.load(file)\n",
    "\n",
    "trainset, kernel_str = dataset['best_trainingsets'], dataset['best_CPKL_str']\n",
    "testset = dataset['testset']\n",
    "\n",
    "Best_GPModel_HF = {}\n",
    "for trainset_, kernel_str_ in zip(trainset, kernel_str): # i_samples\n",
    "    train_x, train_y = trainset_[1].float(), trainset_[0].float()\n",
    "    i_sample = train_x.shape[0]\n",
    "\n",
    "    # Construct a backbone GPModel\n",
    "    best_model_ = reconstruct_backbone_model(train_x, train_y, kernel_str_, options)\n",
    "\n",
    "    # 2. Update initial model using parameter values learned    \n",
    "    fn_sv = dir_fitted_result + f\"/Best_{fn_str[0]}_{fn_str[-1].split('.')[0]}_N{i_sample}_Case{i_load}.pth\" # for learned parameters\n",
    "    state_dict = torch.load(fn_sv) # Load learned values\n",
    "    best_model_.load_state_dict(state_dict)\n",
    "    \n",
    "    Best_GPModel_HF[f\"{fn_str[0]}_{fn_str[-1].split('.')[0]}_N{i_sample}_Case{i_load}\"] = best_model_\n",
    "\n",
    "    # Make prediction\n",
    "    test_x, test_y = testset[1].float(), testset[0].float()\n",
    "\n",
    "    print(fn)\n",
    "    mae, mse = plot_result(best_model_, test_x, test_y, plot_on = False)\n",
    "    print(f\"MAE : {mae:.3f} // MSE : {mse:.3f}\")\n",
    "    print(\"%%%%%%%\" * 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Complimentary Data-fusion: Multi-fidelity approach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_fn_lists = [\n",
    "    'simplebeam_LF0_Case0',\n",
    "    'simplebeam_LF1_Case0',\n",
    "    'simplebeam_LF2_Case0',\n",
    "    'slab1_LF_Case0',\n",
    "    'slab1_LF_Case1',\n",
    "    ]\n",
    "\n",
    "for del_fn in del_fn_lists:\n",
    "    del Best_GPModel_LF[del_fn]\n",
    "\n",
    "Best_GPModel_LF.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import max_error as MAE\n",
    "\n",
    "options = {'training_iter': 100, 'max_depth': 1,\n",
    "            'standardize': True, 'tolerance': 1e-3}\n",
    "\n",
    "data_type = 'MF'\n",
    "rand_seed0, plot_on = 1234, False\n",
    "\n",
    "for fn, LF_model in Best_GPModel_LF.items():\n",
    "    fn = fn.replace('LF', 'HF')\n",
    "\n",
    "    fn_list = dir_trainset + '/' + f'{fn}.pkl'\n",
    "    with open(fn_list, 'rb') as file:\n",
    "        dataset = pickle.load(file)\n",
    "\n",
    "    fn_splited = fn.replace('HF', data_type)\n",
    "    fn_splited_ = fn_splited.split('_')\n",
    "\n",
    "    testset = dataset['testset']\n",
    "    train_set_all_mae, train_set_all_mse = np.full((n_rep,1),np.nan), np.full((n_rep,1),np.nan)\n",
    "    \n",
    "    i_samples = []\n",
    "    for all_trainingsets in dataset['all_trainingsets']: # No. of Samples\n",
    "        model_list = []\n",
    "        train_set_mae, train_set_mse = [], []\n",
    "        best_trainingsets, worst_trainingsets = [], []\n",
    "        best_kernel_types, worst_kernel_types = [], []\n",
    "        \n",
    "        for trainset in all_trainingsets: # repetition : 10\n",
    "            # Step 1: LF-GP model\n",
    "            train_xe, train_ye = deepcopy(trainset[-1]).float(), deepcopy(trainset[0]).float()\n",
    "            test_xe, test_ye = deepcopy(testset[-1]).float(), deepcopy(testset[0]).float()                \n",
    "            \n",
    "            ypred_mu_LF, ypred_var = LF_model.predict(train_xe)\n",
    "            ypred_mu_LF_test, ypred_var = LF_model.predict(test_xe) # For computing metrics (MAE & MSE) for GPModel with GP-mapping\n",
    "\n",
    "            # Step #2: GP Mapping from LF to HF\n",
    "            if train_xe.ndim == 1: # 1D\n",
    "                train_x_aug = torch.concat([train_xe.reshape(-1, 1), torch.tensor(ypred_mu_LF).reshape(-1, 1)], axis = 1)\n",
    "                test_x_aug = torch.concat([test_xe.reshape(-1, 1), torch.tensor(ypred_mu_LF_test).reshape(-1, 1)], axis = 1)\n",
    "            else: # above 2D\n",
    "                train_x_aug = torch.concat([train_xe, torch.tensor(ypred_mu_LF).reshape(-1, 1)], axis = 1)\n",
    "                test_x_aug = torch.concat([test_xe, torch.tensor(ypred_mu_LF_test).reshape(-1, 1)], axis = 1)\n",
    "            \n",
    "            trainset_aug = train_ye, train_x_aug\n",
    "            testset_aug = test_ye, test_x_aug\n",
    "            \n",
    "            Best_MF1_model, best_models, best_metrics, best_index = run_experimental_data_for_CPKL(trainset_aug, testset_aug, rand_seed0, options, plot_on)\n",
    "            \n",
    "            ypred_mu_MF1, ypred_var = Best_MF1_model.predict(train_x_aug)\n",
    "            ypred_mu_MF1_test, ypred_var = Best_MF1_model.predict(test_x_aug) # For computing metrics (MAE & MSE) for GPModel fir discrepancy\n",
    "            \n",
    "            # Step #3: GP for model discrepancy\n",
    "            y_disc = train_ye - ypred_mu_MF1\n",
    "            y_test_disc = test_ye - ypred_mu_MF1_test\n",
    "\n",
    "            trainset_disc = y_disc, train_xe\n",
    "            testset_disc = y_test_disc, test_xe\n",
    "\n",
    "            Best_MFdisc_model, best_models, best_metrics, best_index = run_experimental_data_for_CPKL(trainset_disc, testset_disc, rand_seed0, options, plot_on)\n",
    "            ypred_mu_MFdisc, ypred_var = Best_MFdisc_model.predict(train_xe)\n",
    "            ypred_mu_MFdisc_test, ypred_var = Best_MFdisc_model.predict(test_xe)\n",
    "\n",
    "            best_model_lists = [LF_model, Best_MF1_model, Best_MFdisc_model]\n",
    "\n",
    "            ypred_mu_MF = ypred_mu_MF1 + ypred_mu_MFdisc\n",
    "            ypred_mu_MF_test = ypred_mu_MF1_test + ypred_mu_MFdisc_test\n",
    "\n",
    "            mae, mse = MAE(test_ye.numpy(), ypred_mu_MF_test), MSE(test_ye.numpy(), ypred_mu_MF_test)\n",
    "\n",
    "            # Append GPModel\n",
    "            model_list.append(best_model_lists)\n",
    "            train_set_mae.append(mae.item())\n",
    "            train_set_mse.append(mse.item())\n",
    "        \n",
    "        ind_sample = train_xe.shape[0]\n",
    "        i_samples.append(ind_sample)\n",
    "\n",
    "        # Find indices of best and worst model\n",
    "        train_set_mae = np.array(train_set_mae).reshape(-1, 1)\n",
    "        train_set_mse = np.array(train_set_mse).reshape(-1, 1)\n",
    "\n",
    "        ind_best = np.array(train_set_mae).argmin()\n",
    "        ind_worst = np.array(train_set_mae).argmax()\n",
    "        print(f\"[{fn_splited_[0] + '_' + fn_splited_[1]}] # No. of samples: {ind_sample}, MAE: {train_set_mae[ind_best]}, MSE: {train_set_mse[ind_best]}\")\n",
    "\n",
    "        # Save best and worst GPModels of GP-mapping\n",
    "        fn_sv = f\"Best_{fn_splited_[0]}_{fn_splited_[1]}_GPmapping_N{ind_sample}_{fn_splited_[-1].split('.')[0]}.pth\"\n",
    "        torch.save(model_list[ind_best][1].state_dict(), f'{dir_fitted_result}/{fn_sv}')\n",
    "\n",
    "        fn_sv = f\"Worst_{fn_splited_[0]}_{fn_splited_[1]}_GPmapping_N{ind_sample}_{fn_splited_[-1].split('.')[0]}.pth\"\n",
    "        torch.save(model_list[ind_worst][1].state_dict(), f'{dir_fitted_result}/{fn_sv}')\n",
    "\n",
    "        # Save best and worst GPModels of discrepancy\n",
    "        fn_sv = f\"Best_{fn_splited_[0]}_{fn_splited_[1]}_disc_N{ind_sample}_{fn_splited_[-1].split('.')[0]}.pth\"\n",
    "        torch.save(model_list[ind_worst][2].state_dict(), f'{dir_fitted_result}/{fn_sv}')\n",
    "\n",
    "        fn_sv = f\"Worst_{fn_splited_[0]}_{fn_splited_[1]}_disc_N{ind_sample}_{fn_splited_[-1].split('.')[0]}.pth\"\n",
    "        torch.save(model_list[ind_worst][2].state_dict(), f'{dir_fitted_result}/{fn_sv}')\n",
    "\n",
    "        best_kernel_types.append([model_list[ind_best][1].kernel_type, model_list[ind_best][2].kernel_type])\n",
    "        worst_kernel_types.append([model_list[ind_worst][1].kernel_type, model_list[ind_worst][2].kernel_type])\n",
    "\n",
    "        # Append training data & metrics            \n",
    "        best_trainingsets.append(all_trainingsets[ind_best])\n",
    "        worst_trainingsets.append(all_trainingsets[ind_worst])\n",
    "\n",
    "        if np.isnan(train_set_all_mae).all():\n",
    "            train_set_all_mae, train_set_all_mse = train_set_mae, train_set_mse\n",
    "        else:\n",
    "            train_set_all_mae = np.concatenate([train_set_all_mae, train_set_mae], axis = 1)\n",
    "            train_set_all_mse = np.concatenate([train_set_all_mse, train_set_mse], axis = 1)\n",
    "\n",
    "    df_mae = pd.DataFrame(train_set_all_mae)\n",
    "    df_mae.columns = i_samples\n",
    "    df_mse = pd.DataFrame(train_set_all_mse)\n",
    "    df_mse.columns = i_samples\n",
    "    \n",
    "    df_mae.to_csv(dir_trainset + '/' + f\"MAE_{fn_splited_[0]}_{fn_splited_[1]}_{fn_splited_[-1].split('.')[0]}.csv\")\n",
    "    df_mse.to_csv(dir_trainset + '/' + f\"MSE_{fn_splited_[0]}_{fn_splited_[1]}_{fn_splited_[-1].split('.')[0]}.csv\")\n",
    "\n",
    "    # Save trainingset\n",
    "    fn_train_sv = f\"{fn_splited_[0]}_{fn_splited_[1]}_{fn_splited_[-1].split('.')[0]}.pkl\"\n",
    "    values_saved = {\n",
    "        'best_trainingsets': best_trainingsets, 'worst_trainingsets': worst_trainingsets, \n",
    "        'all_trainingsets': all_trainingsets, 'testset': testset,\n",
    "        'best_CPKL_str': best_kernel_types, 'worst_CPKL_str': worst_kernel_types,\n",
    "        }\n",
    "    \n",
    "    with open(dir_trainset + '/' + fn_train_sv, 'wb') as file:\n",
    "        pickle.dump(values_saved, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CPKL_GPytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
