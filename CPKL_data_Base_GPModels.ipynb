{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# plt.style.use(['dark_background', 'bmh'])\n",
    "plt.style.use(['seaborn-v0_8-whitegrid', 'bmh'])\n",
    "font = {'family' : 'monospace',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 12}\n",
    "plt.rc('font', **font)\n",
    "plt.rc('axes', facecolor='w')\n",
    "plt.rc('figure', facecolor='w')\n",
    "plt.rc('figure', figsize= (6, 3), dpi=150)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "dir_fitted_result = './fitted_GPModels'\n",
    "os.makedirs(dir_fitted_result, exist_ok = True)\n",
    "\n",
    "dir_trainset = './trainset'\n",
    "os.makedirs(dir_trainset, exist_ok = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Examples for sample size**\n",
    "- For 1d-Dataset: `torch.Size([20]) torch.Size([20]) torch.Size([100]) torch.Size([100])`  \n",
    "- For 2d-dataset: `torch.Size([20, 2]) torch.Size([20]) torch.Size([100, 2]) torch.Size([100])`  \n",
    "- For nd-dataset: `torch.Size([20, n]) torch.Size([20]) torch.Size([100, n]) torch.Size([100])`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Define functions for case studies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.kernels import RBFKernel, LinearKernel, RQKernel, MaternKernel, CosineKernel, PeriodicKernel\n",
    "\n",
    "def define_base_kernels(train_x, traindata_info):\n",
    "\n",
    "    # Set active dimensions\n",
    "    action_dims = torch.tensor([i for i in range(0, train_x.dim())])\n",
    "\n",
    "    lengthscale_prior = gpytorch.priors.NormalPrior(\n",
    "        traindata_info['med_dist_x'], traindata_info['max_dist_x'] - traindata_info['min_dist_x'])\n",
    "    variance_prior = gpytorch.priors.NormalPrior(\n",
    "        traindata_info['med_dist_y'], traindata_info['max_dist_y'] - traindata_info['min_dist_y'])\n",
    "    periodlength_prior = gpytorch.priors.NormalPrior(\n",
    "        traindata_info['med_dist_x'], traindata_info['max_dist_x'] - traindata_info['min_dist_x'])\n",
    "\n",
    "    period_length_constraint = gpytorch.constraints.Interval(\n",
    "        0.5*traindata_info['min_dist_x'], traindata_info['max_dist_x']*1.5)\n",
    "\n",
    "    for i in range(0, train_x.dim()):\n",
    "        if i == 0:\n",
    "            lin_ken = LinearKernel(active_dims=i, variance_prior=variance_prior, variance_constraint=gpytorch.constraints.Interval(\n",
    "                0.5*traindata_info['min_dist_y'], traindata_info['max_dist_y']*1.5))\n",
    "\n",
    "            # Samller values of period_length means higher frequency in signals\n",
    "            cos_ken = CosineKernel(active_dims=i, period_length_prior=periodlength_prior, period_length_constraint=gpytorch.constraints.Interval(\n",
    "                0.5 * traindata_info['min_dist_x'], traindata_info['max_dist_x'] * 1.5))\n",
    "            cos_ken1 = CosineKernel(active_dims=i, period_length_prior=periodlength_prior,\n",
    "                                    period_length_constraint=gpytorch.constraints.LessThan(0.5*traindata_info['min_dist_x']))\n",
    "\n",
    "        else:\n",
    "            lin_ken *= lin_ken\n",
    "            cos_ken *= cos_ken\n",
    "            cos_ken1 *= cos_ken1\n",
    "\n",
    "    # define the kernels\n",
    "    base_kernels = {\n",
    "        'rbf': lambda: RBFKernel(active_dims=action_dims, ard_num_dims=len(action_dims), lengthscale_prior=lengthscale_prior),\n",
    "        'lin': lambda: lin_ken,\n",
    "        # 'cos': lambda: cos_ken,\n",
    "        # 'cos1': lambda: cos_ken1,\n",
    "        # 'per': lambda: PeriodicKernel(active_dims=action_dims, ard_num_dims=len(action_dims),\n",
    "        #                             period_length_prior=periodlength_prior, lengthscale_prior=lengthscale_prior,\n",
    "        #                             period_length_constraint = period_length_constraint),\n",
    "        'rq': lambda: RQKernel(active_dims=action_dims, ard_num_dims=len(action_dims), lengthscale_prior=lengthscale_prior,),\n",
    "        'mat12': lambda: MaternKernel(nu=0.5, active_dims=action_dims, ard_num_dims=len(action_dims), lengthscale_prior=lengthscale_prior),\n",
    "        'mat32': lambda: MaternKernel(nu=1.5, active_dims=action_dims, ard_num_dims=len(action_dims), lengthscale_prior=lengthscale_prior),\n",
    "        'mat52': lambda: MaternKernel(nu=2.5, active_dims=action_dims, ard_num_dims=len(action_dims), lengthscale_prior=lengthscale_prior),\n",
    "    }\n",
    "\n",
    "    # base_likelihood = lambda: gpytorch.likelihoods.GaussianLikelihood(noise_constraint=gpytorch.constraints.Positive())\n",
    "\n",
    "    def base_likelihood(): return gpytorch.likelihoods.GaussianLikelihood(\n",
    "        noise_constraint=gpytorch.constraints.GreaterThan(1e-3))\n",
    "\n",
    "    # Assume: likelihood noise constraint was defined so that 1e-4 is within range.\n",
    "    likelihood0 = base_likelihood()\n",
    "    if 0:\n",
    "        # Some small value, but don't make it too small or numerical performance will suffer. I recommend 1e-4.\n",
    "        likelihood0.noise = 1e-4\n",
    "        # Mark that we don't want to train the noise.\n",
    "        likelihood0.noise_covar.raw_noise.requires_grad_(False)\n",
    "\n",
    "    return base_kernels, likelihood0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Util_CPKL import *\n",
    "\n",
    "def run_experimental_data_for_CPKL(trainset, testset, rand_seed0, options, plot_result_on = False):\n",
    "    # Run CPKL\n",
    "    multi_restart_option = {}\n",
    "    multi_restart_option['sampling'] = 'random'\n",
    "    multi_restart_option['n_eval'] = 20\n",
    "    \n",
    "    # Assign input and output from tuples\n",
    "    train_y_all, train_x_all = trainset\n",
    "    test_y_all, test_x_all  = testset\n",
    "\n",
    "    train_x, test_x = train_x_all.float(), test_x_all.float()\n",
    "    train_y, test_y =  train_y_all.float(), test_y_all.float()\n",
    "    \n",
    "    # get information trainint set for initializing hyper-parameters\n",
    "    traindata_info = initialize_hyperparameters(train_x, train_y)\n",
    "    multi_restart_option['traindata_info'] = traindata_info\n",
    "    \n",
    "    # Define base_kernels\n",
    "    base_kernels, likelihood0 = define_base_kernels(train_x, traindata_info)\n",
    "\n",
    "    best_model, best_models, best_metrics, best_index = RUN_CPKL(train_x, train_y, likelihood0, base_kernels,\n",
    "                                                    test_x=test_x, test_y=test_y, multi_restart_option=multi_restart_option,\n",
    "                                                    fix_value_learned=True, max_depth=options['max_depth'], rand_seed0=rand_seed0,\n",
    "                                                    training_iter=options['training_iter'], standardize=options['standardize'], \n",
    "                                                    tolerance=options['tolerance'], plot_intermediate_on=False)\n",
    "    \n",
    "    best_metrics = torch.tensor(best_metrics)\n",
    "    \n",
    "    mae, mse = plot_result(best_model, test_x, test_y, plot_on = plot_result_on)\n",
    "    print(f\"MAE : {mae:.3f} // MSE : {mse:.3f}\")\n",
    "    print(\"%%%%%%%\" * 3)\n",
    "\n",
    "    return best_model, best_models, best_metrics, best_index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training options**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {'training_iter': 100, 'max_depth': 15,\n",
    "            'standardize': True, 'tolerance': 1e-3}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Construct GPModels using whole LF-data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data_type = 'LF'\n",
    "fn_lists_LF = [\n",
    "    f'simplebeam_strain_{data_type}0.csv',\n",
    "    f'simplebeam_strain_{data_type}1.csv',\n",
    "    f'simplebeam_strain_{data_type}2.csv',\n",
    "    f'slab1_strain_{data_type}.csv',\n",
    "    f'slab2_strain_{data_type}.csv',\n",
    "    f'Yongsu_strain_{data_type}.csv',\n",
    "]\n",
    "\n",
    "if 1:\n",
    "    rand_seed0, plot_on = 1234, False\n",
    "\n",
    "    for fn in fn_lists_LF:\n",
    "        fn_str = fn.split('_')\n",
    "\n",
    "        # Import HF-data\n",
    "        df = pd.read_csv('./Data/' + fn)\n",
    "        if df.shape[1] == 2: # 1d\n",
    "            i_loads, input_dim = np.arange(df.shape[1] - 1), 1\n",
    "        else:\n",
    "            i_loads, input_dim = np.arange(df.shape[1] - 2), 2\n",
    "\n",
    "        print(f\"List for Loading Case: {i_loads}\")\n",
    "\n",
    "        for i_load in i_loads:\n",
    "            # Set testdata\n",
    "            if input_dim == 1: # 1D\n",
    "                trainset = (torch.tensor(df.iloc[:, i_load].values), torch.tensor(df.iloc[:, -input_dim].values))\n",
    "                testset = (torch.tensor(df.iloc[:, i_load].values), torch.tensor(df.iloc[:, -input_dim].values))\n",
    "            else: # 2D\n",
    "                testset = (torch.tensor(df.iloc[:, i_load].values), torch.tensor(df.iloc[:, -input_dim:].values))\n",
    "                trainset = (torch.tensor(df.iloc[:, i_load].values), torch.tensor(df.iloc[:, -input_dim:].values))\n",
    "\n",
    "            # Fit GPModel using subsets of HF-data\n",
    "            print(f\"# Case : {i_load}\") # df_tmp.shape[0]\n",
    "            \n",
    "            plot_on = True\n",
    "            best_model, best_models, best_metrics, best_index = run_experimental_data_for_CPKL(deepcopy(trainset), deepcopy(testset), rand_seed0, options, plot_on)\n",
    "            mae, mse = best_metrics[best_index, :]\n",
    "            \n",
    "            # Save best and worst GPModels\n",
    "            fn_sv = f\"Best_{fn_str[0]}_{fn_str[-1].split('.')[0]}_Case{i_load}.pth\"\n",
    "            torch.save(best_model.state_dict(), dir_fitted_result + f'/{fn_sv}')\n",
    "            \n",
    "            # Save trainingset\n",
    "            fn_train_sv = f\"{fn_str[0]}_{fn_str[-1].split('.')[0]}_Case{i_load}.pkl\"\n",
    "            values_saved = {\n",
    "                'trainingset': trainset, 'testset': testset, 'CPKL_str': best_model.kernel_type\n",
    "                }\n",
    "            \n",
    "            with open(dir_trainset + '/' + fn_train_sv, 'wb') as file:\n",
    "                pickle.dump(values_saved, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_best_model(fn_train_sv, fn_sv):\n",
    "    ############################################\n",
    "    # 1. Import dataset\n",
    "    ############################################\n",
    "    # Load datasets\n",
    "    with open(fn_train_sv, 'rb') as file:\n",
    "        dataset = pickle.load(file)\n",
    "\n",
    "    trainset, testset = dataset['trainingset'], dataset['testset']\n",
    "    train_y, train_x = dataset['trainingset'][0].float(), dataset['trainingset'][1].float()\n",
    "\n",
    "    # get information trainint set for initializing hyper-parameters\n",
    "    traindata_info = initialize_hyperparameters(train_x, train_y)\n",
    "\n",
    "    # Define base_kernels\n",
    "    base_kernels, likelihood0 = define_base_kernels(train_x, traindata_info)\n",
    "\n",
    "    kernel_type = dataset['CPKL_str']\n",
    "    kernel_type_splitted = kernel_type.replace('(', '')\n",
    "    kernel_type_splitted = kernel_type_splitted.replace(')', '')\n",
    "    kernel_type_splitted = kernel_type_splitted.replace('+', ' + ')\n",
    "    kernel_type_splitted = kernel_type_splitted.replace('*', ' * ')\n",
    "    kernel_type_splitted = kernel_type_splitted.split(' ')\n",
    "\n",
    "    # Define Learned kernels => it can be single or compositional kernel\n",
    "    if len(kernel_type_splitted) == 1: # Single kernel\n",
    "        kernel_ = base_kernels[kernel_type_splitted]()\n",
    "        \n",
    "    else: # Composition using two more kernels\n",
    "        kernel_ = base_kernels[kernel_type_splitted[0]]()\n",
    "        for key in kernel_type_splitted[1:]:\n",
    "            if key in ['*', '+']:\n",
    "                compotional_rule = key\n",
    "            else:\n",
    "                if compotional_rule == '+':\n",
    "                    kernel_ = kernel_ + base_kernels[key]()\n",
    "                else:\n",
    "                    kernel_ = kernel_ * base_kernels[key]()\n",
    "\n",
    "    # Construct GPModel based on the kernel learned before\n",
    "    best_model_ = ExactGPModel(deepcopy(train_x), deepcopy(train_y), kernel_, kernel_type, deepcopy(likelihood0), options).float()\n",
    "    \n",
    "    ############################################\n",
    "    # 2. Import parameter values learned\n",
    "    ############################################\n",
    "    state_dict = torch.load(fn_sv)\n",
    "    best_model_.load_state_dict(state_dict)\n",
    "\n",
    "    return best_model_, trainset, testset\n",
    "\n",
    "# filename for dataset\n",
    "fn_train_sv = dir_trainset + f\"/{fn_str[0]}_{fn_str[-1].split('.')[0]}_Case{i_load}.pkl\"\n",
    "# filename for learned parameters\n",
    "fn_sv = dir_fitted_result + f\"/Best_{fn_str[0]}_{fn_str[-1].split('.')[0]}_Case{i_load}.pth\"\n",
    "\n",
    "# fn_train_sv, fn_sv = dir_trainset + '/simplebeam_LF0_Case0.pkl', dir_fitted_result + '/Best_simplebeam_LF0_Case0.pth'\n",
    "best_model_, trainset, testset = reconstruct_best_model(fn_train_sv, fn_sv)\n",
    "\n",
    "# Make prediction\n",
    "test_x, test_y = testset[1].float(), testset[0].float()\n",
    "mae, mse = plot_result(best_model_, test_x, test_y, plot_on = True)\n",
    "print(f\"MAE : {mae:.3f} // MSE : {mse:.3f}\")\n",
    "print(\"%%%%%%%\" * 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Construct GPModels using only HF-data (i.e. subsets)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data_type = 'HF'\n",
    "fn_lists_HF = [\n",
    "    f'simplebeam_strain_{data_type}0.csv',\n",
    "    # f'simplebeam_strain_{data_type}1.csv',\n",
    "    # f'simplebeam_strain_{data_type}2.csv',\n",
    "    # f'slab1_strain_{data_type}.csv',\n",
    "    f'slab2_strain_{data_type}.csv',\n",
    "    # f'Yongsu_strain_{data_type}.csv',\n",
    "]\n",
    "\n",
    "ratio_min, ratio_max = 0.3, 0.7\n",
    "i_sample_min, i_sample_max = 6, 10\n",
    "rand_seed0, plot_on = 1234, False\n",
    "n_rep = 10\n",
    "\n",
    "if 1:\n",
    "    for fn in fn_lists_HF:\n",
    "        fn_str = fn.split('_')\n",
    "\n",
    "        # Import HF-data\n",
    "        df = pd.read_csv('./Data/' + fn)\n",
    "        if df.shape[1] == 2: # 1d\n",
    "            i_loads, input_dim = np.arange(df.shape[1] - 1), 1\n",
    "        else:\n",
    "            i_loads, input_dim = np.arange(df.shape[1] - 2), 2\n",
    "\n",
    "        print(f\"List for Loading Case: {i_loads}\")\n",
    "\n",
    "        for i_load in i_loads:\n",
    "            # Set testdata\n",
    "            if input_dim == 1:\n",
    "                testset = (torch.tensor(df.iloc[:, i_load].values), torch.tensor(df.iloc[:, -input_dim].values)) # 1D\n",
    "            else:\n",
    "                testset = (torch.tensor(df.iloc[:, i_load].values), torch.tensor(df.iloc[:, -input_dim:].values)) # 1D\n",
    "\n",
    "            # Loop for generating trainingset via stratified sampling\n",
    "            trainingset_all = []\n",
    "            train_set_all_mae, train_set_all_mse = np.full((n_rep,1),np.nan), np.full((n_rep,1),np.nan)\n",
    "            if input_dim == 1:\n",
    "                i_samples = np.unique(np.linspace(df.shape[0] * ratio_min, df.shape[0] * ratio_max, 5, dtype = int))\n",
    "            else:\n",
    "                i_samples = np.unique(np.arange(i_sample_min, i_sample_max + 1, dtype = int))\n",
    "\n",
    "            print(f\"List for # Samples: {i_samples}\")\n",
    "\n",
    "            for ind_sample in i_samples: # increase trainingset ratio\n",
    "                if input_dim == 1:\n",
    "                    # Get minimal and maximal inputs\n",
    "                    xmin_, xmax_ = testset[-1].min(), testset[-1].max()\n",
    "                    \n",
    "                    # Generate stratified intervals (cut_interval)\n",
    "                    cut_interval = np.linspace(xmin_, xmax_, ind_sample + 1)\n",
    "                    cut_interval[0], cut_interval[-1] = - np.inf, np.inf\n",
    "\n",
    "                    df['cut'] = pd.cut(df['x'], bins = cut_interval, labels = [i for i in range(cut_interval.shape[0] - 1)])\n",
    "                    tmp = df.groupby('cut', group_keys=False)\n",
    "                    \n",
    "                    train_set_mae, train_set_mse = [], []\n",
    "                    train_set, model_list = [], []\n",
    "\n",
    "                    for i_rep in range(n_rep): # repeat random sampling based on stratified intervals\n",
    "                        df_tmp = tmp.apply(lambda x: x.sample(1, replace=True, random_state = rand_seed0 * i_rep))\n",
    "                        df_tmp.drop('cut', axis=1, inplace=True)\n",
    "                        \n",
    "                        trainset = (torch.tensor(df_tmp.iloc[:, i_load].values), torch.tensor(df_tmp.iloc[:, -input_dim].values))\n",
    "\n",
    "                        # Fit GPModel using subsets of HF-data\n",
    "                        print(f\"[{i_rep + 1}/{n_rep}] # sample : {ind_sample} // # Case : {i_load}\") # df_tmp.shape[0]\n",
    "                        best_model, best_models, best_metrics, best_index = run_experimental_data_for_CPKL(deepcopy(trainset), deepcopy(testset), rand_seed0, plot_on)\n",
    "                        mae, mse = best_metrics[best_index, :]\n",
    "\n",
    "                        # Append GPModel\n",
    "                        model_list.append(best_model)\n",
    "                        train_set_mae.append(mae.item())\n",
    "                        train_set_mse.append(mse.item())\n",
    "                        train_set.append(df_tmp)\n",
    "\n",
    "                else: # 2D\n",
    "                        np.random.seed(rand_seed0) # for reproducibility\n",
    "                        if fn_str[0] == 'slab1': # LF-data at x = 300 was not measured\n",
    "                            df_tmp = df.iloc[3:, :]\n",
    "                            \n",
    "                        else:\n",
    "                            df_tmp = df.iloc[:, :]\n",
    "\n",
    "                        train_set_mae, train_set_mse = [], []\n",
    "                        train_set, model_list = [], []\n",
    "\n",
    "                        for i_rep in range(n_rep): # repeat random sampling based on stratified intervals\n",
    "                            ind_train = np.random.choice(df_tmp.shape[0], ind_sample, replace=False)\n",
    "                            trainset = (torch.tensor(df_tmp.iloc[ind_train, i_load].values), torch.tensor(df_tmp.iloc[ind_train, -input_dim:].values))\n",
    "                            \n",
    "                            plot_on = True\n",
    "                            \n",
    "                            # Fit GPModel using subsets of HF-data\n",
    "                            print(f\"[{i_rep + 1}/{n_rep}] # sample : {ind_sample} // # Case : {i_load}\") # df_tmp.shape[0]\n",
    "                            best_model, best_models, best_metrics, best_index = run_experimental_data_for_CPKL(deepcopy(trainset), deepcopy(testset), rand_seed0, plot_on)\n",
    "                            mae, mse = best_metrics[best_index, :]\n",
    "\n",
    "                            # Append GPModel\n",
    "                            model_list.append(best_model)\n",
    "                            train_set_mae.append(mae.item())\n",
    "                            train_set_mse.append(mse.item())\n",
    "                            train_set.append(df_tmp.iloc[ind_train, :])\n",
    "\n",
    "                # Find indices of best and worst model\n",
    "                train_set_mae = np.array(train_set_mae).reshape(-1, 1)\n",
    "                train_set_mse = np.array(train_set_mse).reshape(-1, 1)\n",
    "\n",
    "                ind_best = np.array(train_set_mae).argmin()\n",
    "                ind_worst = np.array(train_set_mae).argmax()\n",
    "\n",
    "                # Save best and worst GPModels\n",
    "                fn_sv = f\"Best_{fn_str[0]}_{fn_str[-1].split('.')[0]}_N{ind_sample}_Case{i_load}.pth\"\n",
    "                torch.save(model_list[ind_best].state_dict(), f'{dir_fitted_result}/{fn_sv}')\n",
    "\n",
    "                fn_sv = f\"Worst_{fn_str[0]}_{fn_str[-1].split('.')[0]}_N{ind_sample}_Case{i_load}.pth\"\n",
    "                torch.save(model_list[ind_worst].state_dict(), f'{dir_fitted_result}/{fn_sv}')\n",
    "                \n",
    "                model_list[ind_best]\n",
    "                model_list[ind_worst]\n",
    "\n",
    "                # Append training data & metrics\n",
    "                trainingset_all.append(train_set)\n",
    "                if np.isnan(train_set_all_mae).all():\n",
    "                    train_set_all_mae, train_set_all_mse = train_set_mae, train_set_mse\n",
    "                else:\n",
    "                    train_set_all_mae = np.concatenate([train_set_all_mae, train_set_mae], axis = 1)\n",
    "                    train_set_all_mse = np.concatenate([train_set_all_mse, train_set_mse], axis = 1)\n",
    "\n",
    "        df_mae = pd.DataFrame(train_set_all_mae)\n",
    "        df_mae.columns = i_samples\n",
    "        df_mse = pd.DataFrame(train_set_all_mse)\n",
    "        df_mse.columns = i_samples\n",
    "\n",
    "        df_mae.to_csv(dir_trainset + '/' + f\"MAE_{fn_str[0]}_{fn_str[-1].split('.')[0]}_Case{i_load}.csv\")\n",
    "        df_mse.to_csv(dir_trainset + '/' + f\"MSE_{fn_str[0]}_{fn_str[-1].split('.')[0]}_Case{i_load}.csv\")\n",
    "\n",
    "        # Save trainingset\n",
    "        fn_train_sv = f\"{fn_str[0]}_{fn_str[-1].split('.')[0]}_Case{i_load}.pkl\"\n",
    "        values_saved = {\n",
    "            'trainingset_all': trainingset_all, 'testset': testset,\n",
    "            }\n",
    "        \n",
    "        with open(dir_trainset + '/' + fn_train_sv, 'wb') as file:\n",
    "            pickle.dump(values_saved, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Complimentary Data-fusion: Multi-fidelity approach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_dataset_of_fitted_GPModels(dataset_fn):\n",
    "#     with open(dataset_fn, 'rb') as f:\n",
    "#         dict = pickle.load(f)\n",
    "\n",
    "#     trainingset = dict['trainingset']\n",
    "#     testset = dict['testset']\n",
    "#     testset\n",
    "\n",
    "# def get_result_of_fitted_GPModels(model_fn):\n",
    "#     with open(model_fn, 'rb') as f:\n",
    "#         dict = pickle.load(f)\n",
    "\n",
    "#     trainingset = dict['trainingset']\n",
    "#     testset = dict['testset']\n",
    "#     testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "\n",
    "# data_type = 'LF'\n",
    "# fn_lists_LF = [\n",
    "#     f'simplebeam_strain_{data_type}0.csv',\n",
    "#     f'simplebeam_strain_{data_type}1.csv',\n",
    "#     f'simplebeam_strain_{data_type}2.csv',\n",
    "#     f'slab1_strain_{data_type}.csv',\n",
    "#     f'slab2_strain_{data_type}.csv',\n",
    "#     f'Yongsu_strain_{data_type}.csv',\n",
    "# ]\n",
    "\n",
    "# # Import GPModels fitted by LF-data\n",
    "# for fn in fn_lists_LF:\n",
    "#     fn_splited = fn.split('_')\n",
    "#     fn_target = f\"{fn_splited[0]}_{fn_splited[-1].split('.')[0]}\"\n",
    "#     # Check # of cases (loading => i_load)\n",
    "#     LF_model_fn_lists = glob.glob(dir_fitted_result + f'/Best_{fn_target}_*.pth')\n",
    "\n",
    "#     for LF_model_fn in LF_model_fn_lists:\n",
    "#         LF_model_fn\n",
    "    \n",
    "#     print(output)\n",
    "#     # break\n",
    "\n",
    "# # Dataset\n",
    "# # dir_trainset\n",
    "\n",
    "# # Fitted GPModels\n",
    "# # dir_fitted_result\n",
    "\n",
    "# # with open(dir_trainset + '/' + fn_train_list[0], 'rb') as f:\n",
    "# #     dict = pickle.load(f)\n",
    "\n",
    "# # trainingset = dict['trainingset']\n",
    "# # testset = dict['testset']\n",
    "# # testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_type = 'HF'\n",
    "fn_lists_HF = [\n",
    "    f'simplebeam_strain_{data_type}0.csv',\n",
    "    f'simplebeam_strain_{data_type}1.csv',\n",
    "    f'simplebeam_strain_{data_type}2.csv',\n",
    "    f'slab1_strain_{data_type}.csv',\n",
    "    f'slab2_strain_{data_type}.csv',\n",
    "    f'Yongsu_strain_{data_type}.csv',\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CPKL_GPytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
